{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REGRESSION\n"
      ],
      "metadata": {
        "id": "lpy0LdAyBz55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.- What is Simple Linear Regression?\n",
        "-  Simple Linear Regression is a statistical technique used to model the relationship between two continuous variables: one independent variable (X) and one dependent variable (Y). It helps us predict the value of Y based on the value of X using a straight-line equation."
      ],
      "metadata": {
        "id": "X2sE2Lt4B6VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the key assumptions of Simple Linear Regression?\n",
        "-  Simple Linear Regression is based on several key assumptions. These assumptions ensure that the model gives valid and reliable results:\n",
        "\n",
        "Linearity\n",
        "\n",
        "The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "\n",
        "That means the data points should roughly follow a straight-line trend.\n",
        "\n",
        "Independence of Errors\n",
        "\n",
        "The residuals (errors) should be independent of each other.\n",
        "\n",
        "There should be no autocorrelation (especially important in time-series data).\n",
        "\n",
        "Homoscedasticity (Constant Variance of Errors)\n",
        "\n",
        "The variance of the residuals should remain constant across all values of X.\n",
        "\n",
        "That means the spread of errors should be even (not widening or narrowing).\n",
        "\n",
        "Normality of Errors\n",
        "\n",
        "The residuals (differences between observed and predicted values) should be normally distributed.\n",
        "\n",
        "This is especially important for confidence intervals and hypothesis testing.\n",
        "\n",
        "No Multicollinearity (applies more to multiple regression)\n",
        "\n",
        "Since simple linear regression only has one independent variable, this assumption isn't relevant here, but important for multiple linear regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "dmKR3kPxCsUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "-  the coefficient ùëö represents the slope or gradient of the line."
      ],
      "metadata": {
        "id": "kJAXeDxMZRDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "- the intercept c represents the Y-intercept ‚Äî the value of Y when ùëã=0."
      ],
      "metadata": {
        "id": "ErmUVoI_aKYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How do we calculate the slope m in Simple Linear Regression?\n",
        "- To calculate the slope ùëö in Simple Linear Regression, we use the formula:\n",
        "\n"
      ],
      "metadata": {
        "id": "kWQsMUoOaxGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "- It helps us find the most accurate line that predicts the dependent variable\n",
        "Y based on the independent variable X.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x8HKbF3SheSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "- The coefficient of determination, denoted as R^2, measures:How well the regression line explains the variation in the dependent variable Y."
      ],
      "metadata": {
        "id": "NhmDzsfyELH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "\n",
        "- Multiple Linear Regression (MLR) is a statistical method used to predict the value of a dependent variable using two or more independent variables.\n",
        "\n",
        "It helps us understand how several factors together influence an outcome."
      ],
      "metadata": {
        "id": "BQR8h1RzE6PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "- The main difference between Simple Linear Regression and Multiple Linear Regression is based on the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "\n",
        "*  Simple Linear Regression is a technique used to model the relationship between one independent variable and one dependent variable using a straight line.\n",
        "*   Multiple Linear Regression is an extension of simple linear regression that models the relationship between two or more independent variables and one dependent variable\n",
        "\n"
      ],
      "metadata": {
        "id": "tiu0vkS6Grui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "1. Linearity:\n",
        "The relationship between the dependent variable and each independent variable is linear.\n",
        "2.   Independence of errors:\n",
        "The residuals (errors) are independent of each other. This means the observations are not correlated.\n",
        "\n",
        "3.  Homoscedasticity:\n",
        "The variance of residuals is constant across all levels of the independent variables.\n",
        "4.  Normality of residuals:\n",
        "The residuals (differences between actual and predicted values) should be approximately normally distributed.\n",
        "5. No multicollinearity:\n",
        "The independent variables should not be highly correlated with each other.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9sBliLXFHO4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "- Heteroscedasticity refers to a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
        "\n",
        "In a properly functioning linear regression model, the residuals should have constant variance ‚Äî this condition is called homoscedasticity. When this condition is violated, it leads to heteroscedasticity.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mmye5CFNIBZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "- Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated. This can make the model unstable and the coefficient estimates unreliable.\n",
        "\n",
        "To improve a model with high multicollinearity, you can use the following methods:"
      ],
      "metadata": {
        "id": "HLjcbiuCIWt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "- Categorical variables need to be transformed into numerical form before being used in regression models. The most common techniques are:\n",
        "1. One-Hot Encoding\n",
        "2. Label Encoding:\n",
        "3. Ordinal Encoding:\n",
        "4. Binary Encoding:\n",
        "5. Target Encoding (Mean Encoding):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bHUL0RAdIyHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "- Interaction terms in Multiple Linear Regression are used to model the situation where the effect of one independent variable on the dependent variable depends on the value of another independent variable."
      ],
      "metadata": {
        "id": "Kxz8qwxpJ5e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "\n",
        "In Simple Linear Regression, the intercept represents the predicted value of the dependent variable when the single independent variable is equal to zero.\n",
        "\n",
        "In Multiple Linear Regression, the intercept represents the predicted value of the dependent variable when all independent variables are equal to zero.\n",
        "\n",
        "The key difference is that:\n",
        "*  In Simple Linear Regression, the intercept is easier to interpret and often has a meaningful value.\n",
        "*  In Multiple Linear Regression, the intercept may not be meaningful if zero is not a realistic or valid value for all predictors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xRmCTgiQLeXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "- In regression analysis, the slope represents the rate of change of the dependent variable with respect to an independent variable.\n",
        "\n",
        "It shows how much the predicted value of the dependent variable (Y) will increase or decrease when the independent variable (X) increases by one unit, assuming all other variables remain constant."
      ],
      "metadata": {
        "id": "LnsT6kbNN8TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "- The intercept in a regression model represents the expected value of the dependent variable (Y) when all independent variables (X) are equal to zero.\n",
        "\n",
        "It provides a baseline or starting point for the regression line or plane, giving context to how the independent variables influence the outcome."
      ],
      "metadata": {
        "id": "YKSO0VuwOIbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "- While R¬≤ (coefficient of determination) measures how well a regression model explains the variation in the dependent variable, it has several limitations when used alone to evaluate model performance.\n",
        "\n",
        "\n",
        "1. Does not indicate model accuracy\n",
        "2. Ignores overfitting\n",
        "3. Does not show causality\n",
        "4. Not useful for non-linear models\n",
        "5. Sensitive to outliers\n",
        "6. No information about individual variable importance\n",
        "\n"
      ],
      "metadata": {
        "id": "sn7cKN_POwHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "- A large standard error for a regression coefficient indicates that the estimate of that coefficient is not precise. It means the coefficient could vary widely if the model were applied to different samples of data.\n",
        "\n",
        "\n",
        "\n",
        "* The coefficient is less reliable.\n",
        "* There is more uncertainty about the true effect of the corresponding independent variable on the dependent variable.\n",
        "* The variable may not be statistically significant, especially if the standard error is large relative to the size of the coefficient.  \n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fSTQHAt_Qovj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "- Heteroscedasticity occurs when the variance of residuals (errors) in a regression model is not constant across all levels of the independent variables.\n",
        "--  **Identification in Residual Plots**\n",
        "\n",
        "** Heteroscedasticity can be identified by plotting the residuals (errors) against the predicted (fitted) values.\n",
        "\n",
        "1. If the residuals form a pattern, such as a funnel shape, cone shape, or systematic curve, this indicates heteroscedasticity.\n",
        "\n",
        "2. If the residuals appear randomly scattered around zero with no clear pattern, the model likely satisfies the assumption of homoscedasticity (constant variance).\n",
        "\n",
        "** Why It‚Äôs Important to Address Heteroscedasticity.\n",
        "\n",
        "\n",
        "\n",
        "1. It violates a key assumption of linear regression.\n",
        "2. It leads to biased or incorrect standard errors, which affects\n",
        "* confidence interval\n",
        "* p value\n",
        "* Hypothesis testing\n",
        "3. It can result in unreliable predictions and misleading interpretations of the model.\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Enqn8914Z4DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "\n",
        "- If a Multiple Linear Regression model shows a high R¬≤ but a low adjusted R¬≤, it means that although the model appears to explain a large portion of the variance in the dependent variable, this may be misleading. The high R¬≤ suggests that the model fits the data well, but the low adjusted R¬≤ indicates that some of the independent variables included in the model are not actually contributing meaningfully to the prediction. This often happens when too many predictors are added, including irrelevant ones, which artificially inflate the R¬≤ without truly improving the model. Adjusted R¬≤ accounts for the number of predictors and penalizes the model for adding variables that do not improve the explanatory power. Therefore, this situation suggests the model may be overfitted and should be simplified by removing unnecessary or unhelpful variables."
      ],
      "metadata": {
        "id": "l6fXlBZxgiGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "- It is important to scale variables in Multiple Linear Regression because it ensures that all independent variables are on the same scale, which improves the performance and interpretability of the model. When variables have very different units or ranges (for example, age in years and income in lakhs), the model may give undue importance to variables with larger values simply due to their scale, not because they are more significant. Scaling helps in avoiding this imbalance. Moreover, scaling is especially important when regularization techniques like Ridge or Lasso regression are used, as these methods are sensitive to the scale of the variables. Although the basic Multiple Linear Regression algorithm can still run without scaling, the coefficients may be misleading and difficult to interpret, so scaling helps create a more stable and balanced model."
      ],
      "metadata": {
        "id": "8HDbXHAIhqVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        "- Polynomial Regression is a type of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial. Unlike Simple Linear Regression, which fits a straight line to the data, Polynomial Regression fits a curved line, making it useful when the data shows a non-linear trend. In this method, higher-order terms of the independent variable (like X¬≤, X¬≥, etc.) are added to the model to capture the curvature in the relationship. For example, a quadratic regression includes X¬≤, while a cubic regression includes both X¬≤ and X¬≥. Polynomial Regression can provide a better fit for complex patterns in the data, but it must be used carefully to avoid overfitting, especially with higher-degree polynomials."
      ],
      "metadata": {
        "id": "qLaE2Kqvh496"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How does polynomial regression differ from linear regression?\n",
        "\n",
        "- Polynomial Regression differs from Linear Regression in the way it models the relationship between the independent and dependent variables. Linear Regression assumes a straight-line relationship and includes only the original independent variable (X) in the equation, such as Y = mX + c. In contrast, Polynomial Regression models a curved relationship by including higher-degree terms of the independent variable, such as X¬≤, X¬≥, and so on. For example, a second-degree polynomial regression would have the form Y = aX¬≤ + bX + c. This allows Polynomial Regression to capture more complex, non-linear patterns in the data that Linear Regression cannot. While Linear Regression fits a line, Polynomial Regression fits a curve, which makes it more flexible but also more prone to overfitting if not used carefully."
      ],
      "metadata": {
        "id": "Ptdglb3hiEj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        "- Polynomial Regression is used when the relationship between the independent and dependent variables is non-linear and cannot be accurately captured by a straight line. It is suitable for situations where the data shows a clear curve, such as U-shaped or inverted U-shaped patterns. This type of regression is commonly applied in fields like physics, biology, economics, and engineering, where variables may interact in a non-linear way. For example, it can be used to model the growth rate of a population, the path of a moving object, or the effect of temperature on chemical reactions. Polynomial Regression helps improve model accuracy in such cases by fitting a curved line that better represents the trend in the data."
      ],
      "metadata": {
        "id": "Nys_aOgeiXtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "- The general equation for Polynomial Regression is an extension of the linear regression equation that includes higher-degree powers of the independent variable. It is written as:\n",
        "Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + Œ≤‚ÇÉX¬≥ + ... + Œ≤‚ÇôX‚Åø + Œµ,\n",
        "where Y is the dependent variable, X is the independent variable, Œ≤‚ÇÄ is the intercept, Œ≤‚ÇÅ to Œ≤‚Çô are the coefficients for each power of X, n is the degree of the polynomial, and Œµ represents the error term. This equation allows the model to fit a curved line to the data, making it useful when the relationship between X and Y is non-linear."
      ],
      "metadata": {
        "id": "JJU6PzKcii8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "- Yes, Polynomial Regression can be applied to multiple variables. In such cases, the model includes not only the original independent variables but also their higher-degree terms and interactions between them. For example, if there are two variables, X‚ÇÅ and X‚ÇÇ, the model can include terms like X‚ÇÅ¬≤, X‚ÇÇ¬≤, and X‚ÇÅ¬∑X‚ÇÇ. This allows the model to capture more complex, non-linear relationships involving more than one predictor. This technique is known as multivariable or multivariate polynomial regression and is especially useful when the outcome is influenced by a combination of variables in a non-linear way. However, including too many polynomial terms can make the model overly complex and lead to overfitting, so careful selection of terms is important."
      ],
      "metadata": {
        "id": "SsXpdhPjixq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "- Polynomial Regression, while useful for modeling non-linear relationships, has several limitations. One major limitation is the risk of overfitting, especially when a high-degree polynomial is used, which can cause the model to fit the noise in the data instead of the actual trend. Another limitation is that polynomial models can become very sensitive to small changes in data, especially near the edges, leading to large prediction errors. Additionally, as the degree of the polynomial increases, the model becomes more complex and harder to interpret. Polynomial Regression also requires careful feature scaling when used with multiple variables, and it may not perform well if the underlying data pattern is not actually polynomial. In such cases, alternative models like decision trees or support vector machines may be more appropriate."
      ],
      "metadata": {
        "id": "jBp9dtB9kI7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "- When selecting the degree of a polynomial in regression, several methods can be used to evaluate the model fit. The most common method is to look at the R¬≤ and adjusted R¬≤ values, where a higher adjusted R¬≤ indicates a better balance between model complexity and accuracy. Cross-validation, especially k-fold cross-validation, is another important technique used to test how well the model performs on unseen data, helping to avoid overfitting. Residual plots can also be examined to check for patterns; if residuals show a systematic pattern, the degree may be too low or too high. Additionally, Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) are often used as numerical indicators of prediction error, and lower values suggest a better fit. By comparing these metrics for different degrees of polynomial, one can choose the optimal model that is both accurate and generalizes well"
      ],
      "metadata": {
        "id": "b94SF8_0kcru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.  Why is visualization important in polynomial regression?\n",
        "\n",
        "- Visualization is important in Polynomial Regression because it helps to clearly understand how well the model fits the data, especially since the relationships involved are non-linear. By plotting the original data points along with the polynomial curve, we can visually inspect whether the curve appropriately follows the pattern in the data or if it is overfitting or underfitting. A well-fitted polynomial curve should smoothly capture the underlying trend without being too wavy or rigid. Visualization also helps to communicate the model‚Äôs behavior to others in a simple and intuitive way, making it easier to explain complex patterns. Additionally, by looking at residual plots alongside the fitted curve, we can detect any remaining patterns or inconsistencies that the model might have missed, guiding further model improvement."
      ],
      "metadata": {
        "id": "4_I1tiB9kntz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "-  Polynomial Regression is implemented in Python using libraries like scikit-learn. First, the data is prepared and split into input (X) and output (y) variables. Then, the PolynomialFeatures class from sklearn.preprocessing is used to transform the original features into polynomial features of the desired degree. After transformation, a LinearRegression model is applied to this new set of features. The model is then trained using the fit() method, and predictions are made using predict(). Finally, performance metrics like R¬≤ score or Mean Squared Error can be used to evaluate the model. Visualization using matplotlib helps to display the polynomial curve along with the original data points, providing a clear view of the fit."
      ],
      "metadata": {
        "id": "eUxOpYO9kzSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EN2cyf8VBbO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ddbf15-5a01-4315-eea5-d47032fff512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HdXRQqWxIVrJ"
      }
    }
  ]
}